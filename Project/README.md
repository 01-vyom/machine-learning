# Project - Sign Language Recognition Detection Using Deep Learning

Vision-based gesture recognition is crucial in helping people with impaired speech and hearing. It is a challenging area of sign language translation that requires processing on a rather limited yet complex dataset. For this study, Word-Level American Sign Language (WLASL) video dataset was used that contains over 2000 glosses. It is one of the largest public ASL datasets to facilitate word-level sign recognition research. We have done a comparative study of various sign language recognition (SLR) systems using video/image-based Deep Learning models to understand their capacity to hold information.
Isolated sign words recognition system has been implemented using pre-trained I3D weights by Carreira & others. Our results were similar to that in the reference study showing appearance-based model achieve up to 63.9% at top-10 accuracy on 2000 glosses, demonstrating the validity and challenges of our datasets.

- [Proposal](https://github.com/01-vyom/machine-learning/blob/main/Project/Sign_Language_Recognition_using_Deep_Learning_Proposal.pdf) for the problem statement.
- [Paper](https://github.com/01-vyom/machine-learning/blob/main/Project/Sign_Language_Recognition_using_Deep_Learning_Paper.pdf) for the problem statement.
- Code adopted from the open source implementation by [1](https://github.com/dxli94/WLASL).